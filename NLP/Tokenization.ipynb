{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4249ab4",
   "metadata": {},
   "source": [
    "The input in natural language processing is text. The data collection for this text happens from a lot of sources. This requires a lot of cleaning and processing before the data can be used for analysis.\n",
    "\n",
    "**These are some of the methods of processing the data in NLP:**\n",
    "\n",
    "* Tokenization\n",
    "* Stop words removal\n",
    "* Stemming\n",
    "* Normalization\n",
    "* Lemmatization\n",
    "* Parts of speech tagging\n",
    "\n",
    "# Tokenization \n",
    "\n",
    "* Tokenization helps to convert the paragraph into sentences and words. Converted words in a form of **list**.  \n",
    "\n",
    "* Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\n",
    "\n",
    "* For example, the text “It is raining” can be tokenized into ‘It’, ‘is’, ‘raining’. \n",
    "* There are different methods and libraries available to perform tokenization. NLTK, Gensim, Keras are some of the libraries that can be used to accomplish the task.\n",
    "\n",
    "* Tokenization can be done to either separate words or sentences. If the text is split into words using some separation technique it is called word tokenization and same    separation done for sentences is called sentence tokenization.\n",
    "\n",
    "* There are various tokenization techniques available which can be applicable based on the language and purpose of modeling. Below are a few of the tokenization techniques used in NLP.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*FTEu803GEsNrNslvY1RbXQ.png\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "**White Space Tokenization:**\n",
    "\n",
    "This is the simplest tokenization technique. Given a sentence or paragraph it tokenizes into words by splitting the input whenever a white space in encountered. This is the fastest tokenization technique but will work for languages in which the white space breaks apart the sentence into meaningful words. Example: English.\n",
    "\n",
    "**Dictionary Based Tokenization:**\n",
    "\n",
    "In this method the tokens are found based on the tokens already existing in the dictionary. If the token is not found, then special rules are used to tokenize it. It is an advanced technique compared to whitespace tokenizer.\n",
    "\n",
    "**Rule Based Tokenization:**\n",
    "\n",
    "In this technique a set of rules are created for the specific problem. The tokenization is done based on the rules. For example creating rules bases on grammar for particular language.\n",
    "\n",
    "**Regular Expression Tokenizer:**\n",
    "\n",
    "This technique uses regular expression to control the tokenization of text into tokens. Regular expression can be simple to complex and sometimes difficult to comprehend. This technique should be preferred when the above methods does not serve the required purpose. It is a rule based tokenizer.\n",
    "\n",
    "**Penn TreeBank Tokenization:**\n",
    "\n",
    "Tree bank is a corpus created which gives the semantic and syntactical annotation of language. Penn Treebank is one of the largest treebanks which was published. This technique of tokenization separates the punctuation, clitics (words that occur along with other words like I’m, don’t) and hyphenated words together.\n",
    "\n",
    "**Spacy Tokenizer:**\n",
    "\n",
    "This is a modern technique of tokenization which faster and easily customizable. It provides the flexibility to specify special tokens that need not be segmented or need to be segmented using special rules. Suppose you want to keep $ as a separate token, it takes precedence over other tokenization operations.\n",
    "\n",
    "**Moses Tokenizer:**\n",
    "\n",
    "This is a tokenizer which is advanced and is available before Spacy was introduced. It is basically a collection of complex normalization and segmentation logic which works very well for structured language like English.\n",
    "\n",
    "**Subword Tokenization:**\n",
    "\n",
    "This tokenization is very useful for specific application where sub words make significance. In this technique the most frequently used words are given unique ids and less frequent words are split into sub words and they best represent the meaning independently. \n",
    "\n",
    "For example if the word few is appearing frequently in the text it will be assigned a unique id, where fewer and fewest which are rare words and are less frequent in the text will be split into sub words like few, er, and est. This helps the language model not to learn fewer and fewest as two separate words. This allows to identify the unknown words in the data set during training. There are different types of subword tokenization and they are given below and Byte-Pair Encoding and WordPiece will be discussed briefly.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*7_s0e2RuWz5_0GYwqQzFlQ.png\" width=\"600\"/></center>\n",
    "\n",
    "1. Byte-Pair Encoding (BPE)\n",
    "2. WordPiece\n",
    "3. Unigram Language Model\n",
    "4. SentencePiece\n",
    "\n",
    "This technique is based on the concepts in information theory and compression. BPE uses Huffman encoding for tokenization meaning it uses more embedding or symbols for representing less frequent words and less symbols or embedding for more frequently used words.\n",
    "\n",
    "**Byte-Pair Encoding (BPE):**\n",
    "\n",
    "The BPE tokenization is bottom up sub word tokenization technique. The steps involved in BPE algorithm is given below.\n",
    "\n",
    "Starts with splitting the input words into single unicode characters and each of them corresponds to a symbol in the final vocabulary.\n",
    "Find the most frequent occurring pair of symbols from the current vocabulary.\n",
    "Add this to the vocabulary and size of vocabulary increases by one.\n",
    "Repeat steps ii and iii till the defined number of tokens are built or no new combination of symbols exist with required frequency.\n",
    "\n",
    "**WordPiece:**\n",
    "\n",
    "WordPiece is similar to BPE techniques expect the way the new token is added to the vocabulary. BPE considers the token with most frequent occurring pair of symbols to merge into the vocabulary. While WordPiece considers the frequency of individual symbols also and based on below count it merges into the vocabulary.\n",
    "\n",
    "`Count (x, y) = frequency of (x, y) / frequency (x) * frequency (y)`\n",
    "\n",
    "The pair of symbols with maximum count will be considered to merge into vocabulary. So it allows rare tokens to be included into vocabulary as compared to BPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945221c-7236-450d-b913-5176983ce92e",
   "metadata": {},
   "source": [
    "Let's do the tokenization with different methods: \n",
    "\n",
    "## 1. NLTK: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa49012-b602-4e62-9ecf-933d22cc795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #natural languages tool kit(library)\n",
    "\n",
    "nltk.download() # It makes sure we download everything in the nltk library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf90880",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its high-level built in data structures, combined with dynamic typing and dynamic binding, make it very attractive for Rapid Application Development, as well as for use as a scripting or glue language to connect existing components together. Python's simple, easy to learn syntax emphasizes readability and therefore reduces the cost of program maintenance. Python supports modules and packages, which encourages program modularity and code reuse. The Python interpreter and the extensive standard library are available in source or binary form without charge for all major platforms, and can be freely distributed.Often, programmers fall in love with Python because of the increased productivity it provides. Since there is no compilation step, the edit-test-debug cycle is incredibly fast. Debugging Python programs is easy: a bug or bad input will never cause a segmentation fault. Instead, when the interpreter discovers an error, it raises an exception. When the program doesn't catch the exception, the interpreter prints a stack trace. A source level debugger allows inspection of local and global variables, evaluation of arbitrary expressions, setting breakpoints, stepping through the code a line at a time, and so on. The debugger is written in Python itself, testifying to Python's introspective power. On the other hand, often the quickest way to debug a program is to add a few print statements to the source: the fast edit-test-debug cycle makes this simple approach very effective.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7728c",
   "metadata": {},
   "source": [
    "* In order to convert paragraph into sentences we use (sent_tokenize()) Function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f42fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_corpus = nltk.sent_tokenize(corpus) \n",
    "#Sentences tokenize (paragraph into sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5109d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python is an interpreted, object-oriented, high-level programming language with dynamic semantics.',\n",
       " 'Its high-level built in data structures, combined with dynamic typing and dynamic binding, make it very attractive for Rapid Application Development, as well as for use as a scripting or glue language to connect existing components together.',\n",
       " \"Python's simple, easy to learn syntax emphasizes readability and therefore reduces the cost of program maintenance.\",\n",
       " 'Python supports modules and packages, which encourages program modularity and code reuse.',\n",
       " 'The Python interpreter and the extensive standard library are available in source or binary form without charge for all major platforms, and can be freely distributed.Often, programmers fall in love with Python because of the increased productivity it provides.',\n",
       " 'Since there is no compilation step, the edit-test-debug cycle is incredibly fast.',\n",
       " 'Debugging Python programs is easy: a bug or bad input will never cause a segmentation fault.',\n",
       " 'Instead, when the interpreter discovers an error, it raises an exception.',\n",
       " \"When the program doesn't catch the exception, the interpreter prints a stack trace.\",\n",
       " 'A source level debugger allows inspection of local and global variables, evaluation of arbitrary expressions, setting breakpoints, stepping through the code a line at a time, and so on.',\n",
       " \"The debugger is written in Python itself, testifying to Python's introspective power.\",\n",
       " 'On the other hand, often the quickest way to debug a program is to add a few print statements to the source: the fast edit-test-debug cycle makes this simple approach very effective.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_corpus  # results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c75597",
   "metadata": {},
   "source": [
    "* In order to convert paragraph into words we use (word_tokenize()) Function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6691cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_corpus = nltk.word_tokenize(corpus)\n",
    "# word tokeniztion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e326a48",
   "metadata": {},
   "source": [
    "**This is the first step in text preprocessing, either you can convert into sentences or words!-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ccbd00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interpreted',\n",
       " ',',\n",
       " 'object-oriented',\n",
       " ',',\n",
       " 'high-level',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'with',\n",
       " 'dynamic',\n",
       " 'semantics',\n",
       " '.',\n",
       " 'Its',\n",
       " 'high-level',\n",
       " 'built',\n",
       " 'in',\n",
       " 'data',\n",
       " 'structures',\n",
       " ',',\n",
       " 'combined',\n",
       " 'with',\n",
       " 'dynamic',\n",
       " 'typing',\n",
       " 'and',\n",
       " 'dynamic',\n",
       " 'binding',\n",
       " ',',\n",
       " 'make',\n",
       " 'it',\n",
       " 'very',\n",
       " 'attractive',\n",
       " 'for',\n",
       " 'Rapid',\n",
       " 'Application',\n",
       " 'Development',\n",
       " ',',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'for',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'scripting',\n",
       " 'or',\n",
       " 'glue',\n",
       " 'language',\n",
       " 'to',\n",
       " 'connect',\n",
       " 'existing',\n",
       " 'components',\n",
       " 'together',\n",
       " '.',\n",
       " 'Python',\n",
       " \"'s\",\n",
       " 'simple',\n",
       " ',',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'syntax',\n",
       " 'emphasizes',\n",
       " 'readability',\n",
       " 'and',\n",
       " 'therefore',\n",
       " 'reduces',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'of',\n",
       " 'program',\n",
       " 'maintenance',\n",
       " '.',\n",
       " 'Python',\n",
       " 'supports',\n",
       " 'modules',\n",
       " 'and',\n",
       " 'packages',\n",
       " ',',\n",
       " 'which',\n",
       " 'encourages',\n",
       " 'program',\n",
       " 'modularity',\n",
       " 'and',\n",
       " 'code',\n",
       " 'reuse',\n",
       " '.',\n",
       " 'The',\n",
       " 'Python',\n",
       " 'interpreter',\n",
       " 'and',\n",
       " 'the',\n",
       " 'extensive',\n",
       " 'standard',\n",
       " 'library',\n",
       " 'are',\n",
       " 'available',\n",
       " 'in',\n",
       " 'source',\n",
       " 'or',\n",
       " 'binary',\n",
       " 'form',\n",
       " 'without',\n",
       " 'charge',\n",
       " 'for',\n",
       " 'all',\n",
       " 'major',\n",
       " 'platforms',\n",
       " ',',\n",
       " 'and',\n",
       " 'can',\n",
       " 'be',\n",
       " 'freely',\n",
       " 'distributed.Often',\n",
       " ',',\n",
       " 'programmers',\n",
       " 'fall',\n",
       " 'in',\n",
       " 'love',\n",
       " 'with',\n",
       " 'Python',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'increased',\n",
       " 'productivity',\n",
       " 'it',\n",
       " 'provides',\n",
       " '.',\n",
       " 'Since',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'compilation',\n",
       " 'step',\n",
       " ',',\n",
       " 'the',\n",
       " 'edit-test-debug',\n",
       " 'cycle',\n",
       " 'is',\n",
       " 'incredibly',\n",
       " 'fast',\n",
       " '.',\n",
       " 'Debugging',\n",
       " 'Python',\n",
       " 'programs',\n",
       " 'is',\n",
       " 'easy',\n",
       " ':',\n",
       " 'a',\n",
       " 'bug',\n",
       " 'or',\n",
       " 'bad',\n",
       " 'input',\n",
       " 'will',\n",
       " 'never',\n",
       " 'cause',\n",
       " 'a',\n",
       " 'segmentation',\n",
       " 'fault',\n",
       " '.',\n",
       " 'Instead',\n",
       " ',',\n",
       " 'when',\n",
       " 'the',\n",
       " 'interpreter',\n",
       " 'discovers',\n",
       " 'an',\n",
       " 'error',\n",
       " ',',\n",
       " 'it',\n",
       " 'raises',\n",
       " 'an',\n",
       " 'exception',\n",
       " '.',\n",
       " 'When',\n",
       " 'the',\n",
       " 'program',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'catch',\n",
       " 'the',\n",
       " 'exception',\n",
       " ',',\n",
       " 'the',\n",
       " 'interpreter',\n",
       " 'prints',\n",
       " 'a',\n",
       " 'stack',\n",
       " 'trace',\n",
       " '.',\n",
       " 'A',\n",
       " 'source',\n",
       " 'level',\n",
       " 'debugger',\n",
       " 'allows',\n",
       " 'inspection',\n",
       " 'of',\n",
       " 'local',\n",
       " 'and',\n",
       " 'global',\n",
       " 'variables',\n",
       " ',',\n",
       " 'evaluation',\n",
       " 'of',\n",
       " 'arbitrary',\n",
       " 'expressions',\n",
       " ',',\n",
       " 'setting',\n",
       " 'breakpoints',\n",
       " ',',\n",
       " 'stepping',\n",
       " 'through',\n",
       " 'the',\n",
       " 'code',\n",
       " 'a',\n",
       " 'line',\n",
       " 'at',\n",
       " 'a',\n",
       " 'time',\n",
       " ',',\n",
       " 'and',\n",
       " 'so',\n",
       " 'on',\n",
       " '.',\n",
       " 'The',\n",
       " 'debugger',\n",
       " 'is',\n",
       " 'written',\n",
       " 'in',\n",
       " 'Python',\n",
       " 'itself',\n",
       " ',',\n",
       " 'testifying',\n",
       " 'to',\n",
       " 'Python',\n",
       " \"'s\",\n",
       " 'introspective',\n",
       " 'power',\n",
       " '.',\n",
       " 'On',\n",
       " 'the',\n",
       " 'other',\n",
       " 'hand',\n",
       " ',',\n",
       " 'often',\n",
       " 'the',\n",
       " 'quickest',\n",
       " 'way',\n",
       " 'to',\n",
       " 'debug',\n",
       " 'a',\n",
       " 'program',\n",
       " 'is',\n",
       " 'to',\n",
       " 'add',\n",
       " 'a',\n",
       " 'few',\n",
       " 'print',\n",
       " 'statements',\n",
       " 'to',\n",
       " 'the',\n",
       " 'source',\n",
       " ':',\n",
       " 'the',\n",
       " 'fast',\n",
       " 'edit-test-debug',\n",
       " 'cycle',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'simple',\n",
       " 'approach',\n",
       " 'very',\n",
       " 'effective',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_corpus  # results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27887ad6-7833-4d20-9e44-6bc89e59d224",
   "metadata": {},
   "source": [
    "## 2. spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b175b2b0-c8ac-4aee-a5ce-f5cf31d9021e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'thinc.neural'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14804/972412525.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\Anaconda_God\\envs\\aravind\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# These are imported as part of the API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'thinc.neural'"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "tokens = tokenizer(\"This is a sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0fa5b-4e67-4dfc-92c9-67d7e8430ac9",
   "metadata": {},
   "source": [
    "## 3. Keras: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a76b713-3c1b-4972-b294-8942bad99266",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interpreted',\n",
       " 'object',\n",
       " 'oriented',\n",
       " 'high',\n",
       " 'level',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'with',\n",
       " 'dynamic',\n",
       " 'semantics',\n",
       " 'its',\n",
       " 'high',\n",
       " 'level',\n",
       " 'built',\n",
       " 'in',\n",
       " 'data',\n",
       " 'structures',\n",
       " 'combined',\n",
       " 'with',\n",
       " 'dynamic',\n",
       " 'typing',\n",
       " 'and',\n",
       " 'dynamic',\n",
       " 'binding',\n",
       " 'make',\n",
       " 'it',\n",
       " 'very',\n",
       " 'attractive',\n",
       " 'for',\n",
       " 'rapid',\n",
       " 'application',\n",
       " 'development',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'for',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'scripting',\n",
       " 'or',\n",
       " 'glue',\n",
       " 'language',\n",
       " 'to',\n",
       " 'connect',\n",
       " 'existing',\n",
       " 'components',\n",
       " 'together',\n",
       " \"python's\",\n",
       " 'simple',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'syntax',\n",
       " 'emphasizes',\n",
       " 'readability',\n",
       " 'and',\n",
       " 'therefore',\n",
       " 'reduces',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'of',\n",
       " 'program',\n",
       " 'maintenance',\n",
       " 'python',\n",
       " 'supports',\n",
       " 'modules',\n",
       " 'and',\n",
       " 'packages',\n",
       " 'which',\n",
       " 'encourages',\n",
       " 'program',\n",
       " 'modularity',\n",
       " 'and',\n",
       " 'code',\n",
       " 'reuse',\n",
       " 'the',\n",
       " 'python',\n",
       " 'interpreter',\n",
       " 'and',\n",
       " 'the',\n",
       " 'extensive',\n",
       " 'standard',\n",
       " 'library',\n",
       " 'are',\n",
       " 'available',\n",
       " 'in',\n",
       " 'source',\n",
       " 'or',\n",
       " 'binary',\n",
       " 'form',\n",
       " 'without',\n",
       " 'charge',\n",
       " 'for',\n",
       " 'all',\n",
       " 'major',\n",
       " 'platforms',\n",
       " 'and',\n",
       " 'can',\n",
       " 'be',\n",
       " 'freely',\n",
       " 'distributed',\n",
       " 'often',\n",
       " 'programmers',\n",
       " 'fall',\n",
       " 'in',\n",
       " 'love',\n",
       " 'with',\n",
       " 'python',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'increased',\n",
       " 'productivity',\n",
       " 'it',\n",
       " 'provides',\n",
       " 'since',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'compilation',\n",
       " 'step',\n",
       " 'the',\n",
       " 'edit',\n",
       " 'test',\n",
       " 'debug',\n",
       " 'cycle',\n",
       " 'is',\n",
       " 'incredibly',\n",
       " 'fast',\n",
       " 'debugging',\n",
       " 'python',\n",
       " 'programs',\n",
       " 'is',\n",
       " 'easy',\n",
       " 'a',\n",
       " 'bug',\n",
       " 'or',\n",
       " 'bad',\n",
       " 'input',\n",
       " 'will',\n",
       " 'never',\n",
       " 'cause',\n",
       " 'a',\n",
       " 'segmentation',\n",
       " 'fault',\n",
       " 'instead',\n",
       " 'when',\n",
       " 'the',\n",
       " 'interpreter',\n",
       " 'discovers',\n",
       " 'an',\n",
       " 'error',\n",
       " 'it',\n",
       " 'raises',\n",
       " 'an',\n",
       " 'exception',\n",
       " 'when',\n",
       " 'the',\n",
       " 'program',\n",
       " \"doesn't\",\n",
       " 'catch',\n",
       " 'the',\n",
       " 'exception',\n",
       " 'the',\n",
       " 'interpreter',\n",
       " 'prints',\n",
       " 'a',\n",
       " 'stack',\n",
       " 'trace',\n",
       " 'a',\n",
       " 'source',\n",
       " 'level',\n",
       " 'debugger',\n",
       " 'allows',\n",
       " 'inspection',\n",
       " 'of',\n",
       " 'local',\n",
       " 'and',\n",
       " 'global',\n",
       " 'variables',\n",
       " 'evaluation',\n",
       " 'of',\n",
       " 'arbitrary',\n",
       " 'expressions',\n",
       " 'setting',\n",
       " 'breakpoints',\n",
       " 'stepping',\n",
       " 'through',\n",
       " 'the',\n",
       " 'code',\n",
       " 'a',\n",
       " 'line',\n",
       " 'at',\n",
       " 'a',\n",
       " 'time',\n",
       " 'and',\n",
       " 'so',\n",
       " 'on',\n",
       " 'the',\n",
       " 'debugger',\n",
       " 'is',\n",
       " 'written',\n",
       " 'in',\n",
       " 'python',\n",
       " 'itself',\n",
       " 'testifying',\n",
       " 'to',\n",
       " \"python's\",\n",
       " 'introspective',\n",
       " 'power',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'hand',\n",
       " 'often',\n",
       " 'the',\n",
       " 'quickest',\n",
       " 'way',\n",
       " 'to',\n",
       " 'debug',\n",
       " 'a',\n",
       " 'program',\n",
       " 'is',\n",
       " 'to',\n",
       " 'add',\n",
       " 'a',\n",
       " 'few',\n",
       " 'print',\n",
       " 'statements',\n",
       " 'to',\n",
       " 'the',\n",
       " 'source',\n",
       " 'the',\n",
       " 'fast',\n",
       " 'edit',\n",
       " 'test',\n",
       " 'debug',\n",
       " 'cycle',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'simple',\n",
       " 'approach',\n",
       " 'very',\n",
       " 'effective']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "list_words = text_to_word_sequence(corpus)\n",
    "list_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4d090-3793-4074-aa60-8de207ce48ba",
   "metadata": {},
   "source": [
    "## 4. TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf950de9-f26f-4f83-a195-8b5b59e17918",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'an', 'interpreted', 'object-oriented', 'high-level', 'programming', 'language', 'with', 'dynamic', 'semantics', 'Its', 'high-level', 'built', 'in', 'data', 'structures', 'combined', 'with', 'dynamic', 'typing', 'and', 'dynamic', 'binding', 'make', 'it', 'very', 'attractive', 'for', 'Rapid', 'Application', 'Development', 'as', 'well', 'as', 'for', 'use', 'as', 'a', 'scripting', 'or', 'glue', 'language', 'to', 'connect', 'existing', 'components', 'together', 'Python', \"'s\", 'simple', 'easy', 'to', 'learn', 'syntax', 'emphasizes', 'readability', 'and', 'therefore', 'reduces', 'the', 'cost', 'of', 'program', 'maintenance', 'Python', 'supports', 'modules', 'and', 'packages', 'which', 'encourages', 'program', 'modularity', 'and', 'code', 'reuse', 'The', 'Python', 'interpreter', 'and', 'the', 'extensive', 'standard', 'library', 'are', 'available', 'in', 'source', 'or', 'binary', 'form', 'without', 'charge', 'for', 'all', 'major', 'platforms', 'and', 'can', 'be', 'freely', 'distributed.Often', 'programmers', 'fall', 'in', 'love', 'with', 'Python', 'because', 'of', 'the', 'increased', 'productivity', 'it', 'provides', 'Since', 'there', 'is', 'no', 'compilation', 'step', 'the', 'edit-test-debug', 'cycle', 'is', 'incredibly', 'fast', 'Debugging', 'Python', 'programs', 'is', 'easy', 'a', 'bug', 'or', 'bad', 'input', 'will', 'never', 'cause', 'a', 'segmentation', 'fault', 'Instead', 'when', 'the', 'interpreter', 'discovers', 'an', 'error', 'it', 'raises', 'an', 'exception', 'When', 'the', 'program', 'does', \"n't\", 'catch', 'the', 'exception', 'the', 'interpreter', 'prints', 'a', 'stack', 'trace', 'A', 'source', 'level', 'debugger', 'allows', 'inspection', 'of', 'local', 'and', 'global', 'variables', 'evaluation', 'of', 'arbitrary', 'expressions', 'setting', 'breakpoints', 'stepping', 'through', 'the', 'code', 'a', 'line', 'at', 'a', 'time', 'and', 'so', 'on', 'The', 'debugger', 'is', 'written', 'in', 'Python', 'itself', 'testifying', 'to', 'Python', \"'s\", 'introspective', 'power', 'On', 'the', 'other', 'hand', 'often', 'the', 'quickest', 'way', 'to', 'debug', 'a', 'program', 'is', 'to', 'add', 'a', 'few', 'print', 'statements', 'to', 'the', 'source', 'the', 'fast', 'edit-test-debug', 'cycle', 'makes', 'this', 'simple', 'approach', 'very', 'effective']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob_text = TextBlob(corpus) \n",
    "\n",
    "print(blob_text.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed32fc7-d2a2-4d47-a2a8-8f3bc51f6c6d",
   "metadata": {},
   "source": [
    "## 5. Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af8f2c9-fd8b-451f-a3ab-eec7637d1759",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interpreted',\n",
       " 'object',\n",
       " 'oriented',\n",
       " 'high',\n",
       " 'level',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'with',\n",
       " 'dynamic',\n",
       " 'semantics',\n",
       " 'Its',\n",
       " 'high',\n",
       " 'level',\n",
       " 'built',\n",
       " 'in',\n",
       " 'data',\n",
       " 'structures',\n",
       " 'combined',\n",
       " 'with',\n",
       " 'dynamic',\n",
       " 'typing',\n",
       " 'and',\n",
       " 'dynamic',\n",
       " 'binding',\n",
       " 'make',\n",
       " 'it',\n",
       " 'very',\n",
       " 'attractive',\n",
       " 'for',\n",
       " 'Rapid',\n",
       " 'Application',\n",
       " 'Development',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'for',\n",
       " 'use',\n",
       " 'as',\n",
       " 'a',\n",
       " 'scripting',\n",
       " 'or',\n",
       " 'glue',\n",
       " 'language',\n",
       " 'to',\n",
       " 'connect',\n",
       " 'existing',\n",
       " 'components',\n",
       " 'together',\n",
       " 'Python',\n",
       " 's',\n",
       " 'simple',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'syntax',\n",
       " 'emphasizes',\n",
       " 'readability',\n",
       " 'and',\n",
       " 'therefore',\n",
       " 'reduces',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'of',\n",
       " 'program',\n",
       " 'maintenance',\n",
       " 'Python',\n",
       " 'supports',\n",
       " 'modules',\n",
       " 'and',\n",
       " 'packages',\n",
       " 'which',\n",
       " 'encourages',\n",
       " 'program',\n",
       " 'modularity',\n",
       " 'and',\n",
       " 'code',\n",
       " 'reuse',\n",
       " 'The',\n",
       " 'Python',\n",
       " 'interpreter',\n",
       " 'and',\n",
       " 'the',\n",
       " 'extensive',\n",
       " 'standard',\n",
       " 'library',\n",
       " 'are',\n",
       " 'available',\n",
       " 'in',\n",
       " 'source',\n",
       " 'or',\n",
       " 'binary',\n",
       " 'form',\n",
       " 'without',\n",
       " 'charge',\n",
       " 'for',\n",
       " 'all',\n",
       " 'major',\n",
       " 'platforms',\n",
       " 'and',\n",
       " 'can',\n",
       " 'be',\n",
       " 'freely',\n",
       " 'distributed',\n",
       " 'Often',\n",
       " 'programmers',\n",
       " 'fall',\n",
       " 'in',\n",
       " 'love',\n",
       " 'with',\n",
       " 'Python',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'increased',\n",
       " 'productivity',\n",
       " 'it',\n",
       " 'provides',\n",
       " 'Since',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'compilation',\n",
       " 'step',\n",
       " 'the',\n",
       " 'edit',\n",
       " 'test',\n",
       " 'debug',\n",
       " 'cycle',\n",
       " 'is',\n",
       " 'incredibly',\n",
       " 'fast',\n",
       " 'Debugging',\n",
       " 'Python',\n",
       " 'programs',\n",
       " 'is',\n",
       " 'easy',\n",
       " 'a',\n",
       " 'bug',\n",
       " 'or',\n",
       " 'bad',\n",
       " 'input',\n",
       " 'will',\n",
       " 'never',\n",
       " 'cause',\n",
       " 'a',\n",
       " 'segmentation',\n",
       " 'fault',\n",
       " 'Instead',\n",
       " 'when',\n",
       " 'the',\n",
       " 'interpreter',\n",
       " 'discovers',\n",
       " 'an',\n",
       " 'error',\n",
       " 'it',\n",
       " 'raises',\n",
       " 'an',\n",
       " 'exception',\n",
       " 'When',\n",
       " 'the',\n",
       " 'program',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'catch',\n",
       " 'the',\n",
       " 'exception',\n",
       " 'the',\n",
       " 'interpreter',\n",
       " 'prints',\n",
       " 'a',\n",
       " 'stack',\n",
       " 'trace',\n",
       " 'A',\n",
       " 'source',\n",
       " 'level',\n",
       " 'debugger',\n",
       " 'allows',\n",
       " 'inspection',\n",
       " 'of',\n",
       " 'local',\n",
       " 'and',\n",
       " 'global',\n",
       " 'variables',\n",
       " 'evaluation',\n",
       " 'of',\n",
       " 'arbitrary',\n",
       " 'expressions',\n",
       " 'setting',\n",
       " 'breakpoints',\n",
       " 'stepping',\n",
       " 'through',\n",
       " 'the',\n",
       " 'code',\n",
       " 'a',\n",
       " 'line',\n",
       " 'at',\n",
       " 'a',\n",
       " 'time',\n",
       " 'and',\n",
       " 'so',\n",
       " 'on',\n",
       " 'The',\n",
       " 'debugger',\n",
       " 'is',\n",
       " 'written',\n",
       " 'in',\n",
       " 'Python',\n",
       " 'itself',\n",
       " 'testifying',\n",
       " 'to',\n",
       " 'Python',\n",
       " 's',\n",
       " 'introspective',\n",
       " 'power',\n",
       " 'On',\n",
       " 'the',\n",
       " 'other',\n",
       " 'hand',\n",
       " 'often',\n",
       " 'the',\n",
       " 'quickest',\n",
       " 'way',\n",
       " 'to',\n",
       " 'debug',\n",
       " 'a',\n",
       " 'program',\n",
       " 'is',\n",
       " 'to',\n",
       " 'add',\n",
       " 'a',\n",
       " 'few',\n",
       " 'print',\n",
       " 'statements',\n",
       " 'to',\n",
       " 'the',\n",
       " 'source',\n",
       " 'the',\n",
       " 'fast',\n",
       " 'edit',\n",
       " 'test',\n",
       " 'debug',\n",
       " 'cycle',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'simple',\n",
       " 'approach',\n",
       " 'very',\n",
       " 'effective']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import tokenize \n",
    "\n",
    "list(tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a22c1-35c0-46ec-a6d2-4ccd9ed18068",
   "metadata": {},
   "source": [
    "## 6. Transformer Tokenizers \n",
    "\n",
    "If you are curious about how transformers tokeniers work, you can click this [**link**](https://colab.research.google.com/drive/1v0LTKt0CGFdbjhoDUNhoWGRWpB-khXn4?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5806ef-82d2-44b0-9a65-a0d65978c390",
   "metadata": {},
   "source": [
    "<center><img src=\"https://thumbs.gfycat.com/UncomfortableHatefulKakarikis-size_restricted.gif\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d523e-6096-4d66-ba8e-a21c4247c322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
