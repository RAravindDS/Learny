{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec39f4c-8c4e-4484-a490-960b503570da",
   "metadata": {},
   "source": [
    "<center><h1>Ｗｏｒｄ２ｖｅｃ</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5053747-b06d-4bec-a92d-c6c361955f0f",
   "metadata": {},
   "source": [
    "In NLP, we need to convert our text to numbers, then only we give our input to the model for training. There are lot's of method available for this but all are having some limitations. Today we are going to discuss about one of the method called `word2vec`. It simply means we are going to convert the words to vectors. This vectors have meaning information. Compare to previous methods like `onehotencoder` and `countvectorizer` they don't have any meaning releated to the words but `word2vec` have meaningful infomration about the input words. \n",
    "\n",
    "Let's start from very basic! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f56637-2eba-4e99-9fc5-f66e2460aa7c",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "\n",
    "* Word vectors are also known as **word embeddings**, some of the applications are `sentiment analysis`, `classification of customer feedback`, `semantic analogies`, `machine translation`, `information extraction`, `question answering`. \n",
    "* Whenever you are dealing with text, you first have to find a way to encode the words as numbers. `Word embedding` are a very common technique that allows you to do so.\n",
    "\n",
    "<center><img src=\"images/1.png\" width=\"600\"/></center> \n",
    "<center><img src=\"images/2.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208305f1-0026-44da-9c8b-45cd722313fe",
   "metadata": {},
   "source": [
    "### Representing words to numbers:\n",
    "* We have seen lots of technique used to encode your words to vectors,  **one hot vector**.\n",
    "* There are some limitation in the one-hot-vector, if your vocabulary get larger and larger your column vector increase to more number like 1miliion or more.\n",
    "* Another limitation is that, it don't have any semantic meaning for the word. \n",
    "\n",
    "<center><img src=\"images/3.png\" width=\"500\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81eeaf-58a4-4d9a-a24d-6197cd4e3eac",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "* This method helps to encode the meaning in **relatively low dimensional vector**.\n",
    "* It has **`Embed meaning`** like semantic distance (forest = tree, forest != ticket), and analogies. \n",
    "* we represent word embedding is non zero decimal values. \n",
    "* We will simply build a `n` dimension vector spaces and project our calculated word values.\n",
    "\n",
    "<center><img src=\"images/4.png\" width=\"600\"/></center> \n",
    "\n",
    "From the plot above, you can see that when encoding a word in 2D, similar words tend to be found next to each other. Perhaps the first coordinate represents whether a word is positive or negative. The second coordinate tell you whether the word is abstract or concrete. This is just an example, in the real world you will find embeddings with hundreds of dimensions. You can think of each coordinate as a number telling you something about the word.\n",
    "\n",
    "<center><img src=\"images/5.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9758e9ee-e8ed-4cb3-ad84-670132ffb216",
   "metadata": {},
   "source": [
    "### Create word embeddings \n",
    "* While Creating word embeddigs, the corpus you are using the will affect your word embeddings.\n",
    "* To create a word embeddings, you need two things 1. corpora 2. embedding function. \n",
    "* The corpora should be having contextual meaning like (wikepediia article, law books)\n",
    "*  The context of a word refers to what other words or combination of words tend to occur around that particular word, the context is important as this is what will give meaning to each word embedding. Then only we can catpure the contextual meaning and nuances of the words. \n",
    "* The embedded function helps to create word embeddings, here we will discuss about ML model, it will learn automatically. The model we are going to use called **self-supervised model**, (this is a combination of supervised and unsupervised). \n",
    "* Now, you have good corpora and embedding function, but one thing, before giving your raw text to your embedding function, you need to convert your word to intergers or vectors, we have seen in the represenation. \n",
    "\n",
    "<center><img src=\"images/6.png\" width=\"700\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96446ab-a60f-41dd-b710-6a361ad638d9",
   "metadata": {},
   "source": [
    "### Word embeddding methods: \n",
    "There are lots of word embedding methods available, let's discuss few of them. \n",
    "\n",
    "1. Word2Vec(google, 2013) - uses a shallow(ஆழமற்ற) neural network to create word embeddings. \n",
    "\n",
    "It has 2 methods: \n",
    "\n",
    "* Continuos bag of words(CBOW) - word2vec have two architecures, CBOW is one of this. It helps to find missing word given by surronding words. \n",
    "* Continous skip gram/ skip gram negative sampling(SGNS) - It does reverse teh CBOW. \n",
    "\n",
    "2. Global vectors(standford, 2014) (Glove Don't use any nerual networks) \n",
    "3. Fast Text (Facebook, 2016) \n",
    "* It based on skip gram model, it helps to find the new word (previously unseen words). - support out of vocabulary(OOV)words. \n",
    "\n",
    "----------Some Advanced methods---------\n",
    "\n",
    "4. BERT (Bi directional representation from transformers) (Google, 2018)\n",
    "5. ElMO (Embedding from language models) (Allen Institute of AI, 2018) \n",
    "6. GPT-2 (Generative Pretraining 2) (Open AI, 2018) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f381d5-85c4-4885-a708-98f99effdec2",
   "metadata": {},
   "source": [
    "Here, we are going to discuss about the word2vec. In word2vec we are going to disucss about `CBOW` model. \n",
    "\n",
    "### Continuous bag of words(CBOW): \n",
    "* This is based on Word2vec model, now we will  implementing  this model now. \n",
    "* For training, we forcely remove a word and train our model to predict the word, this helps to train the model for having contextual embeddings. \n",
    "* Here, our ultimate aim to predict the missing words in the sentences. \n",
    "* Let's see how to train the model.\n",
    "* To create a training example you need to give two hyperparametes \n",
    "1. C(context words) = 2 (how many neighbours you need). \n",
    "2. W(Window size) = 5 (count of the center word and context word). \n",
    "\n",
    "\n",
    "<center><img src=\"images/7.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf2a8c-76e8-4f2c-b5fa-9eb79832ee75",
   "metadata": {},
   "source": [
    "It's just an example, but in real time we have a very large amount of data! \n",
    "\n",
    "<center><img src=\"images/8.png\" width=\"700\"/></center>\n",
    "\n",
    "The inputs are context words and outputs are just a center word, we will train a model this for all the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d58808-c8d7-4600-a9f9-7c4b8a6d089f",
   "metadata": {},
   "source": [
    "But before giving our input data to model, we need to do some preprocessing steps! \n",
    "\n",
    "Some of the preprocessing steps are: \n",
    "\n",
    "<center><img src=\"images/9.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd5c6b-6530-4f0c-95df-93ba8653be9c",
   "metadata": {},
   "source": [
    "### Pre Processing step by step! \n",
    "\n",
    "#### Step1: Converting Words into One hot vectors: \n",
    "<center><img src=\"images/11.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f9b206-c851-4150-bef2-0928683fa984",
   "metadata": {},
   "source": [
    "#### Step2: Average of all OneHotVectors: \n",
    "\n",
    "<center><img src=\"images/12.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b599d-2bd6-4f4a-8471-50d9f332feb9",
   "metadata": {},
   "source": [
    "#### Step3: One hot vector of Center word (or) target word: \n",
    "\n",
    "<center><img src=\"images/14.png\" width=\"300\"/></center> \n",
    "\n",
    "**Overview**\n",
    "<center><img src=\"images/13.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8a398-0170-4c29-b011-5e01c25e8c9e",
   "metadata": {},
   "source": [
    "### CBOW architecture: \n",
    "* It is based on shallow dense neural network. This is just a regular feedforward neural network or dense neural network. \n",
    "* It having one hidden layer only!\n",
    "\n",
    "<center><img src=\"images/15.png\" width=\"600\"/></center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4cf1c2-3924-46b9-89c8-fbb672ef6dd6",
   "metadata": {},
   "source": [
    "We know the `Context word vectors` and `Center word one hot vector` (input and real output we know). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c3ac0-3b4e-4f4c-b3b7-482ad5a64d6f",
   "metadata": {},
   "source": [
    "<center><img src=\"images/16.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e28ce-e3b8-4d47-b282-8509cfb5d6ae",
   "metadata": {},
   "source": [
    "In the real world, we will give batch of inputs and get batch of outputs, this pic shows how to intrept the batch inputs and outputs! \n",
    "\n",
    "<center><img src=\"images/17.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240fa90-2eef-4a47-8a50-3c26e98fa8a6",
   "metadata": {},
   "source": [
    "CBOW uses the two `activations` functions. They are `ReLU` and `softmax`. \n",
    "\n",
    "#### ReLU\n",
    "\n",
    "* It is a very simple function. If the value is lesser than 0 it outputs `0`, if the values are greater than zero it outputs the same values as `input`\n",
    "<center><img src=\"images/18.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7054ed5-4fae-4949-a6f9-3e2932626f5d",
   "metadata": {},
   "source": [
    "#### Softmax \n",
    "* It is used for multi class classification, it converts the logits to probabilities. \n",
    "* We know the sum of the probability is `1`. \n",
    "* Steps to calculate the softmax are mentioned below: \n",
    "\n",
    "<center><img src=\"images/19.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44553357-fd3e-427c-9206-cae3c78c58ca",
   "metadata": {},
   "source": [
    "<center><img src=\"images/20.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff35a29-bb66-46f1-b6e5-54b06a8a2c22",
   "metadata": {},
   "source": [
    "#### Loss \n",
    "\n",
    "* In order to predict the actual word, we need to do backpropagation for back prop we need a loss function to tell how much weight I need to update, here we are using the `cross entropy loss function`. \n",
    "\n",
    "<center><img src=\"images/21.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc612cae-f604-4213-88e5-9391a10bbb4a",
   "metadata": {},
   "source": [
    "Let's see how to calculate the `cross entropy loss` step by step~\n",
    "\n",
    "<center><img src=\"images/23.png\" width=\"600\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f84b0-78a4-4f82-bbc4-3f30a2ff0f5d",
   "metadata": {},
   "source": [
    "**We will train this model until get good results! (lower loss value)**\n",
    "\n",
    "Let's see how to get the embeddings from the `trained model`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ba5be-6204-44e3-a301-4cb4ead561c4",
   "metadata": {},
   "source": [
    "### Extracting word embedding from the trained model! \n",
    "* The direct output of the model is not a `word embedding`, we need to do some more functionality there, let's see. \n",
    "* There are three possible ways to extract the word embedding. \n",
    "\n",
    "1. The first method is get each column of the w2 as the word as the word embedding row vector for the corresponding word. \n",
    "\n",
    "<center><img src=\"images/24.png\" width=\"600\"/></center> \n",
    "\n",
    "2. The second method is get each row of the w2 as the word as the word embedding row vector for the corresponding word. \n",
    "<center><img src=\"images/25.png\" width=\"600\"/></center> \n",
    "\n",
    "3. The third method is that average of the first and second method\n",
    "<center><img src=\"images/26.png\" width=\"600\"/></center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d325619-4fa5-46dc-90b0-f402d92386d1",
   "metadata": {},
   "source": [
    "This is how you train and extract the word embedding from the model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
